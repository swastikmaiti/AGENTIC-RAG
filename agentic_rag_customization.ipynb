{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader,Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from pypdf import PdfReader\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "We will be using custom pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('path to a pdf file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this work we will not be using our own custom nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` We will check for reserch paper pdf with keyword abstract and references. If present we will take content starting from abstract till beginning of references, otherwise we will consider the full pdf. The steps for nodes creattion is as follows: ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- merge all sentences into a singe string\n",
    "- convert to lower case character\n",
    "- split into sentence \n",
    "- create nodes of 20 sentences with a overlap of 1 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_1 = ''\n",
    "first_section = \"abstract\"\n",
    "ignore_after = \"references\"\n",
    "pg_no = 0\n",
    "for page in reader.pages:\n",
    "    pg_no += 1\n",
    "    documents_1 += page.extract_text(0)\n",
    "cleaned_string = documents_1.replace('\\n', ' ')\n",
    "cleaned_string = cleaned_string.lower()\n",
    "\n",
    "start_index = cleaned_string.find(first_section)\n",
    "end_index = cleaned_string.rindex(ignore_after)\n",
    "cleaned_string = cleaned_string[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = cleaned_string.split('. ')\n",
    "context_list = []\n",
    "page_number_list = []\n",
    "group_size = 20\n",
    "overlap = 1\n",
    "i = 0 \n",
    "while True:\n",
    "    group = sentence_list[i:i+group_size]\n",
    "    text = '. '.join(group)\n",
    "    if len(text)>10:\n",
    "        context_list.append(text)\n",
    "    i+=group_size-overlap\n",
    "    if i>=len(sentence_list):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=t) for t in context_list]\n",
    "documents_for_summarization = [Document(text=t) for t in context_list[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model='phi3',request_timeout=3600.0,temperature=1)\n",
    "Settings.embed_model = OllamaEmbedding(model_name='nomic-embed-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex,VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(documents)\n",
    "vector_index = VectorStoreIndex(documents_for_summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = vector_index.as_query_engine(streaming=True)\n",
    "summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\",streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = vector_query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = summary_query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Prompt for Phi3\n",
    "\n",
    "### Phi3 from `Microsoft` has a speicific prompting pattern as defined in the [Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_tmpl_str = (\"<|user|>\\n\"\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, answer the query\\n\"\n",
    "    \"Query: {query_str}\"\n",
    "    \" <|end|>\\n\"\n",
    "    \"<|assistant|>\"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "vector_query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_prompt_tmpl_str = (\"<|user|>\\n\"\n",
    "    \"The original query is as follows: {query_str}\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\"\n",
    "    \" <|end|>\\n\"\n",
    "    \"<|assistant|>\"\n",
    ")\n",
    "refine_prompt_tmpl = PromptTemplate(refine_prompt_tmpl_str)\n",
    "\n",
    "vector_query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": refine_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt_tmpl_str = (\"<|user|>\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge, answer the query.\\n\"\n",
    "    \"Query: {query_str}\"\n",
    "    \" <|end|>\\n\"\n",
    "    \"<|assistant|>\"\n",
    ")\n",
    "summary_prompt_tmpl = PromptTemplate(summary_prompt_tmpl_str)\n",
    "\n",
    "summary_query_engine.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": summary_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Query Engine Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters,MetadataFilter\n",
    "from llama_index.core.tools import QueryEngineTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=('Useful for summarization questions')\n",
    ")\n",
    "\n",
    "query_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=('Useful for specific topic based questions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "agent = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,query_tool\n",
    "    ],\n",
    "    verbose=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = agent.query(\"What is attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = agent.query(\"Describe the paper to me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamindex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
